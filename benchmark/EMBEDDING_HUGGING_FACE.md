## Integrating Blackhole into Your LLM Training

Incorporating Blackhole into your LLM training pipeline involves two main components: **input tokenization** and **leveraging a custom embedding layer**.

### 1. Preparing Input Data with `BlackholeTokenizer`

The first and most crucial step is using `BlackholeTokenizer`. This tokenizer is responsible for pre-processing raw text into a format understandable by your Blackhole embedding layer.

**What does `BlackholeTokenizer` do?**

* **Standard Tokenization**: It breaks down text into sub-units (tokens), like words or subwords.
* **Number Detection and Replacement**: When it encounters a number (e.g., "123.45", "-10", "8.0e9"), it replaces it with a special token, for instance, `[NUM]`. This differentiates it from other text tokens.
* **Numerical Metadata Extraction**: Simultaneously with replacing the number, the tokenizer extracts its actual numerical value (as a `float`) and information about its original format (e.g., integer, floating-point, scientific notation).

**Here's how to use it:**

```python
import torch
from blackhole.tokenizer_hugging_face import BlackholeTokenizer, CUSTOM_SPECIAL_TOKENS

# Step 1a: Initialize and train the tokenizer
# Pass a representative corpus of data that contains both text and numbers
# in various formats to train your tokenizer.
tokenizer = BlackholeTokenizer()
sample_texts = [
    "The price is 12.99 USD.",
    "The absolute value is -50.0.",
    "Earth's population is approximately 8.0e9 people."
]
tokenizer.train_tokenizer(sample_texts, vocab_size=5000) # Adjust vocab_size as needed

# Step 1b: Get the ID of the special [NUM] token
# This ID will be needed for configuring the embedding layer.
num_token_id = tokenizer.vocab.get(CUSTOM_SPECIAL_TOKENS["number_token"])
if num_token_id is None:
    raise ValueError(f"Token '{CUSTOM_SPECIAL_TOKENS['number_token']}' not found in vocabulary. Ensure the tokenizer was trained correctly.")

# Step 1c: Use the tokenizer to encode your training data
# When processing data for training, ensure `return_tensors="pt"`
# and that you retrieve all three keys: `input_ids`, `numeric_values`, `numeric_formats`.
text_to_encode = "Today the temperature is 25.5 degrees."
encoded_input = tokenizer(
    text_to_encode,
    padding="max_length",      # Pad to the maximum sequence length
    truncation=True,           # Truncate if the text is too long
    max_length=128,            # Maximum sequence length
    return_tensors="pt"        # Return PyTorch tensors
)

input_ids = encoded_input['input_ids']          # Tensor of token IDs (contains `num_token_id` for numbers)
numeric_values = encoded_input['numeric_values'] # Tensor of actual numerical values (`NaN` for non-numeric tokens)
numeric_formats = encoded_input['numeric_formats'] # Tensor of numerical format IDs (`-1` for non-numeric tokens)

print(f"Encoded input_ids: {input_ids}")
print(f"Extracted numeric_values: {numeric_values}")
print(f"Extracted numeric_formats: {numeric_formats}")
```

In your LLM's training loop, for each batch of data, you'll call the `tokenizer` this way to obtain these three tensors.

---

### 2. Replacing the Standard Embedding Layer with `BlackholeEmbeddings`

Instead of a standard `nn.Embedding` layer or embeddings from the Hugging Face library, you'll use `BlackholeEmbeddings`. This layer takes the data generated by the tokenizer and intelligently combines textual information with rich numerical features.

**How does it work internally?**

1.  **Base Embeddings**: First, it generates standard word, position, and token type embeddings (similar to BERT or GPT).
2.  **Numerical Embedding Generation**: For each token identified by the tokenizer as `[NUM]` and for which it provided a real value (`numeric_values` is not `NaN`), `BlackholeEmbeddings` calculates a set of additional features: logarithm, sign, exponent base 10, a simplified binary representation, and a one-hot encoded format type. These features are then processed through a small neural network (MLP) to create a numerical embedding vector.
3.  **Fusion of Embeddings**: The textual and numerical embeddings are intelligently merged. You can choose a fusion strategy:
    * **"gating"**: The model learns how much weight to give to numerical versus textual information for a given `[NUM]` token. This is a dynamic and adaptive approach.
    * **"add"**: Simple summation of textual and numerical embeddings.
    * **"concat"**: Embeddings are concatenated and then projected to the appropriate dimension via a linear layer.

**Implementing it in your LLM:**

In your LLM's model class (e.g., inheriting from `nn.Module` or `transformers.PreTrainedModel`), initialize `BlackholeConfig` and `BlackholeEmbeddings` within the `__init__` method.

```python
import torch.nn as nn
from typing import Optional
from transformers import PreTrainedModel, PretrainedConfig

# Ensure you have your BlackholeEmbeddings and BlackholeConfig classes accessible
# This typically means your file structure should allow for direct imports.
from blackhole.embadding_hugging_face import BlackholeEmbeddings, BlackholeConfig

# Step 2a: Define the configuration for Blackhole Embeddings
# This configuration will be passed to your embedding layer.
# Ensure `num_token_id` matches the one from your tokenizer!
bh_config = BlackholeConfig(
    vocab_size=tokenizer.vocab_size, # From your tokenizer
    hidden_size=768,                 # Standard embedding dimension for LLMs
    max_position_embeddings=512,     # Maximum sequence length
    pad_token_id=tokenizer.pad_token_id,
    num_token_id=num_token_id,       # [NUM] token ID from the tokenizer
    numeric_feature_dims={           # Configuration for numerical features
        "log_value": 1,
        "sign": 1,
        "exponent": 1,
        "binary_representation": 16, # Example: 16 "bits" for simplified binary representation
        "format_type": 3,            # Example: 3 format types (0:int, 1:float, 2:scientific)
    },
    numeric_embedding_fusion_type="gating", # Choose your preferred fusion type: "gating", "add", or "concat"
    # Add other standard LLM configuration parameters here, e.g.,
    # num_hidden_layers=12,
    # num_attention_heads=12,
    # intermediate_size=3072,
    # hidden_act="gelu",
    # ...
)

# Step 2b: Integrate BlackholeEmbeddings into your LLM model class
class MyBlackholeLLM(PreTrainedModel): # Example for a Hugging Face-style model
    config_class = BlackholeConfig # Link your custom config

    def __init__(self, config: BlackholeConfig):
        super().__init__(config)
        self.config = config

        # Here, you replace the standard embedding layer with BlackholeEmbeddings
        self.embeddings = BlackholeEmbeddings(config)

        # The rest of your LLM architecture (e.g., Transformer layers, language model head)
        # will take the output from `self.embeddings` as input.
        # Example:
        # self.transformer_encoder = SomeTransformerEncoder(config)
        # self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)

        # Initialize weights and apply final processing
        self.post_init()


    def forward(
        self,
        input_ids: torch.Tensor,
        numeric_values: torch.Tensor,
        numeric_formats: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        # ... other arguments your model might need (e.g., labels, encoder_hidden_states)
    ):
        # Step 2c: Call the embedding layer with all necessary data
        # This will produce the combined textual and numerical embeddings.
        embedding_output = self.embeddings(
            input_ids=input_ids,
            numeric_values=numeric_values,
            numeric_formats=numeric_formats,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            # Pass other arguments if your BlackholeEmbeddings needs them
        )

        # `embedding_output` is now a tensor of shape (batch_size, seq_len, hidden_size)
        # containing the enriched numerical embeddings at the appropriate positions.

        # Step 2d: Pass the embedding output to the rest of your model
        # Example:
        # encoder_outputs = self.transformer_encoder(
        #     embedding_output,
        #     attention_mask=attention_mask
        # )
        # sequence_output = encoder_outputs[0] # Assuming first element is the hidden states

        # logits = self.lm_head(sequence_output)
        # return logits

        # For simplicity, returning just the embeddings in this example
        return embedding_output
```

---

## Why Blackhole for Your LLM?

Integrating Blackhole into your LLM is a qualitative leap in its capabilities. Here are the key benefits:

* **Deeper Understanding of Numbers**: Instead of perceiving "123" and "456" as arbitrary, distinct tokens, your model will understand that both are positive integers within a certain magnitude range. This allows it to better differentiate, for example, between "10" and "1000," which is crucial in many contexts.
* **Enhanced Quantitative Reasoning**: The model gains the ability to perform simple arithmetic, compare values, and even identify trends or anomalies in numerical data. For instance, it will be better at answering questions like "How much would five of these products cost if one is 12.99?"
* **More Accurate Text Generation**: When the model needs to generate a number, it will do so with a greater awareness of its value and context. This reduces errors like "Earth has 800 inhabitants" instead of "8 billion."
* **Improved Domain-Specific Performance**: In fields such as finance, science, medicine, or data analysis, where numbers carry critical meaning, the model will be significantly better at processing reports, scientific articles, or statistical data.
* **Reduced Numerical Hallucinations**: With a better understanding of numbers, the model will be less prone to "hallucinating" false numerical data.

By implementing `BlackholeTokenizer` and `BlackholeEmbeddings`, you provide your LLM with a solid foundation for **truly understanding and interacting with the world of numbers**, not just its textual representation. This makes your model significantly more powerful and versatile.