# Blackhole-LLM Embedding Module: Bridging Text and Numbers

-----

## 1\. Introduction and Purpose

The **Blackhole-LLM Embedding Module** is a critical component within the broader Blackhole-LLM architecture, designed to provide comprehensive vector representations (embeddings) for both **textual tokens and numerical values**. While traditional language models primarily focus on textual embeddings, our module introduces a novel approach to numerical representation, addressing the limitations of standard tokenizers when encountering quantitative data.

This module is **not a standalone general-purpose embedding library**, but rather a specialized system deeply integrated with the **Blackhole-LLM Tokenizer**. It works in conjunction with the tokenizer's `<|num|>` token and the accompanying `number_map` to ensure that the LLM receives rich, semantically meaningful, and numerically precise input. This is fundamental to Blackhole-LLM's core objective: **enabling advanced mathematical reasoning and robust structured data processing**.

-----

## 2\. Why Specialized Numerical Embeddings are Essential

As highlighted in the [Blackhole-LLM Tokenizer README](https://github.com/Elwenor/Blackhole-LLM/blob/main/benchmark/TOKENIZER.md), standard tokenizers often fail to preserve the intrinsic value and relationships of numbers. This creates a significant gap for LLMs attempting complex numerical tasks. The Blackhole-LLM Embedding Module addresses this by:

  * **Preserving Numerical Precision:** Instead of treating numbers as mere strings, our module creates embeddings that encode the exact floating-point value through a unique binary representation.
  * **Capturing Numerical Semantics:** It embeds properties like sign, order of magnitude (via bucket encoding), and type (integer, float, hexadecimal, date/time component), providing the model with a deeper understanding of the number's context and meaning.
  * **Enabling Mathematical Operations:** By providing a structured, rich numerical embedding, the LLM is better equipped to perform arithmetic, comparisons, and other mathematical operations, moving beyond simple pattern matching on number sequences.
  * **Handling Special Numerical States:** It robustly handles `None` (for padding), `Infinity`, and `NaN` values, ensuring consistent representation even for edge cases.

The combined power of the Blackhole-LLM Tokenizer and the Embedding Module allows the LLM to process data where text and numbers are intertwined, transforming complex inputs into a format that facilitates sophisticated reasoning.

-----

> [\!NOTE]
>
> ### ðŸ’¡ Design Philosophy: Enhanced Numerical Intelligence ðŸ’¡
>
> The core philosophy behind the Blackhole-LLM Embedding Module is to augment the LLM's understanding of numerical data far beyond what is possible with traditional token-based embeddings. By transforming numbers into dense feature vectors that capture their value, type, and bit-level representation, we aim to instill a form of "numerical intelligence" directly into the model's input layer. This is crucial for applications requiring precise calculations and data interpretation.

-----

## 3\. Key Components and Functionality

The Blackhole-LLM Embedding Module comprises several critical functions and PyTorch `nn.Module` classes:

  * ### `number_embedding_features(val: float, typ: str, dim: int = 128) -> torch.Tensor`

    This core function generates a comprehensive feature vector for a given numerical value. It combines:

      * **Semantic Features:** Sign, zero-ness, type (int, float, hex, date/time component).
      * **Order-of-Magnitude (Bucket) Features:** A one-hot encoding across 19 buckets based on the `log10` of the absolute value, providing a coarse-grained representation of magnitude.
      * **Binary (Bit-level) Features:** A 64-bit representation of the `float64` value, where each bit is mapped to `[-1.0, 1.0]`. This allows for high-precision numerical encoding.
      * **Padding and Special Value Handling:** Uses specific values (`-2.0` for padding, `-3.0` for infinity, `-4.0` for NaN) to robustly represent various states.

  * ### `decode_number_from_features(features: torch.Tensor) -> float`

    A crucial inverse function designed to reconstruct the original numerical value from its feature tensor.

      * **Prioritizes Binary Decoding:** Attempts to decode the number from its 64-bit binary features first, offering high precision.
      * **Fallback to Approximate Decoding:** If binary decoding fails (e.g., due to truncated features or noise), it uses semantic and bucket features to provide an approximate reconstruction of the value.
      * **Handles Special Values:** Correctly decodes padding, infinity, and NaN representations.

  * ### `TokenEmbedding(nn.Module)`

    A standard PyTorch `nn.Embedding` layer responsible for creating dense vector representations for the textual tokens generated by the tokenizer (e.g., words, punctuation, special markers like `<|cap|>`, `<|space|>`).

  * ### `NumberEmbedding(nn.Module)`

    A PyTorch `nn.Module` that takes the `numeric_features` tensor (generated by `number_embedding_features`) and projects it into the model's embedding space. It currently uses a simple `nn.Linear` layer followed by `ReLU` activation for transformation. This layer can be further enhanced with more complex neural architectures if needed.

  * ### `prepare_inputs(tokens: list[str], number_map: dict, dim: int = 128) -> tuple[torch.Tensor, torch.Tensor, dict]`

    (NEW) This utility function streamlines the creation of inputs for models that utilize both `TokenEmbedding` and `NumberEmbedding`.

      * **Vocabulary Construction:** Dynamically builds a vocabulary (`vocab`) from the provided tokens, including special tokens like `<|unk|>`, `<|pad|>`, `<|num|>`.
      * **Token ID Generation:** Converts the list of tokens into a `torch.Tensor` of token IDs, ready for `TokenEmbedding`.
      * **Numerical Feature Tensor Generation:** Creates a `torch.Tensor` of the specified `dim` for numerical features. It populates this tensor with the output of `number_embedding_features` for every `<|num|>` token identified in the `number_map`, filling other positions with a padding value (`-2.0`). This ensures that the numerical embedding tensor has the same sequence length as the token embedding tensor.

-----

## 4\. How to Use

The module is designed to be used in conjunction with the Blackhole-LLM Tokenizer.

### Step 1: Tokenize Text and Get Number Map

First, use the Blackhole-LLM Tokenizer to process your raw text. It will return a list of tokens and a `number_map`.

```python
# Assuming you have the Blackhole-LLM Tokenizer available
from blackhole.tokenizer import BlackholeTokenizer # Placeholder for actual import

tokenizer = BlackholeTokenizer()
text = "The price is 123.45 USD, and the date is 2023-05-22."
tokens, number_map = tokenizer.tokenize_and_map_numbers(text)

print(f"Tokens: {tokens}")
print(f"Number Map: {number_map}")
# Expected:
# Tokens: ['The', '<|space|>', 'price', '<|space|>', 'is', '<|space|>', '<|num|>', '<|space|>', 'USD', ',', '<|space|>', 'and', '<|space|>', 'the', '<|space|>', 'date', '<|space|>', 'is', '<|space|>', '<|num|>', '.']
# Number Map: {6: (123.45, 'float', '123.45'), 19: (2023, 'int_date_comp', '2023-05-22')}
```

### Step 2: Prepare Inputs for Embedding Layers

Use the `prepare_inputs` function from this module to convert the tokenizer's output into the tensors required by `TokenEmbedding` and `NumberEmbedding`.

```python
from blackhole.embedding import prepare_inputs, TokenEmbedding, NumberEmbedding
import torch

embedding_dim = 128 # Define your desired embedding dimension

token_ids, numeric_features_tensor, vocab = prepare_inputs(
    tokens, number_map, dim=embedding_dim
)

print(f"Token IDs shape: {token_ids.shape}")
print(f"Numeric Features Tensor shape: {numeric_features_tensor.shape}")
print(f"Vocabulary size: {len(vocab)}")
```

### Step 3: Create and Use Embedding Layers

Instantiate your `TokenEmbedding` and `NumberEmbedding` layers and pass the prepared input tensors through them.

```python
# Instantiate the embedding layers
vocab_size = len(vocab)
token_embedding_layer = TokenEmbedding(vocab_size, embedding_dim)
number_embedding_layer = NumberEmbedding(input_dim=embedding_dim, output_dim=embedding_dim)

# Get the embeddings
token_embeddings = token_embedding_layer(token_ids)
number_embeddings = number_embedding_layer(numeric_features_tensor)

print(f"Token Embeddings shape: {token_embeddings.shape}") # [1, sequence_length, embedding_dim]
print(f"Number Embeddings shape: {number_embeddings.shape}") # [1, sequence_length, embedding_dim]

# For the Blackhole-LLM architecture, these embeddings would typically be combined
# (e.g., summed, concatenated, or passed through separate attention heads)
# before being fed into subsequent transformer layers.
combined_embeddings = token_embeddings + number_embeddings
print(f"Combined Embeddings shape: {combined_embeddings.shape}")
```

### Example: Decoding a Number from its Features

To verify the quality of the numerical embedding, you can use `decode_number_from_features`:

```python
from blackhole.embedding import decode_number_from_features

# Retrieve the features for the first number (index 6 in the example)
# Note: You'd typically get this from the numeric_features_tensor generated previously
# For demonstration, let's regenerate it directly:
sample_val = 123.45
sample_typ = 'float'
sample_features = number_embedding_features(sample_val, sample_typ, dim=embedding_dim)

decoded_val = decode_number_from_features(sample_features)
print(f"Original value: {sample_val}, Decoded value: {decoded_val}")

# Test with special values
decoded_nan = decode_number_from_features(number_embedding_features(float('nan'), 'float'))
print(f"Original NaN, Decoded: {decoded_nan}")

decoded_inf = decode_number_from_features(number_embedding_features(float('inf'), 'float'))
print(f"Original Inf, Decoded: {decoded_inf}")
```

-----

## 5\. Role within the Blackhole-LLM Architecture

The Embedding Module plays a pivotal role as the **interface between the raw processed input and the transformer layers** of Blackhole-LLM:

1.  **Dual Representation:** It creates two parallel streams of embeddings: one for textual semantics and one for precise numerical values.
2.  **Information Preservation:** Unlike methods that might lose numerical exactitude, this module ensures that critical quantitative data is retained and made available to the LLM.
3.  **Foundation for Reasoning:** By providing distinct yet combinable embeddings, it sets the stage for the LLM's ability to differentiate between text and numbers and apply different reasoning mechanisms accordingly (e.g., symbolic manipulation for numbers, contextual understanding for text).
4.  **Vocabulary Efficiency:** By relying on the `<|num|>` token for numbers, it keeps the textual vocabulary size manageable while offloading the complexity of numerical representation to a dedicated, rich embedding scheme.

-----

## 6\. Current Status and Limitations

This project is currently in an **active architectural development phase**. While our **key innovative componentsâ€”the custom tokenizer system and the numerical embeddings moduleâ€”are functional and being actively refined**, their design and implementation are still subject to **ongoing improvements and potential significant changes**. This means their current state, though operational, is **experimental and not yet optimized for general utility or stability.**

Test implementations of the core language model, which will leverage these innovative components, are under development.

Blackhole-LLM is made public for transparency and to showcase novel architectural solutions. **However, it is not yet intended for production use or for independent execution by external users.** Its primary purpose at this stage is to demonstrate a conceptual approach to LLM architecture.

To validate our innovative components, we've prepared **internal benchmarks and unit tests** that compare the performance of our unique Tokenizer against solutions like GPT-2 Tokenizer and BERT. Further benchmarks specifically for the numerical embedding's ability to preserve and reconstruct values are ongoing.

-----

## 7\. Future Development

  * **Advanced `NumberEmbedding` Architectures:** Exploring more complex neural networks (e.g., small MLPs, attention mechanisms) within the `NumberEmbedding` module to learn more sophisticated transformations of numerical features.
  * **Dynamic Dimensioning:** Allowing for more flexible `dim` configurations based on the complexity of numerical data.
  * **Improved Decoding Accuracy:** Further refining the `decode_number_from_features` logic, especially for edge cases and noisy features, to maximize reconstruction fidelity.
  * **Integration with Training Loop:** Seamless integration into the full Blackhole-LLM training pipeline to measure the practical impact of these embeddings on downstream tasks (e.g., mathematical problem-solving, table understanding).
  * **Quantization Support:** Exploring methods for quantized numerical embeddings for efficient deployment.

-----